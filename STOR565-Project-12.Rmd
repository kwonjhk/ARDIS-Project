---
title: "STOR565-Project-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nnet)
library(e1071)
library(class)
library(readr)
library(randomForest)
library(caret)
library(ranger)

#enable multi-core computing
#only comptaible with OSX/Linux
library(doMC)
registerDoMC()


#import data
img <- 
  read_csv("~/Downloads/STOR565/565_project/img_data_12.csv") %>%
  mutate(y = as.factor(y))

names(img) <- make.names(names(img))

#create test set
set.seed(1)
test = sample(1:nrow(img), 600)
```

Would not recommend running code below since model training/grid search would take too long. Instead, download the 'img_12_models.RData' file and load into the environment directly.


Multinomial logistic regression model using 'nnet'
```{r}
# multinomial logistic regression model
logmod = multinom(y~., data = img[-test,], MaxNWts = 2000, 
                  maxit = 1000)
mean(predict(logmod, img[test,]) != img[test,]$y)
```


Tried to tune linear SVM with the e1071 package, but the computation was too slow.
```{r message = FALSE, warning = FALSE}
#svm.tune = tune(svm, y ~ ., data = img[-test,], kernel = "linear", range = list(cost = c(2^(-8:8))))

#mean(predict(svmmod, img[test,]) != img[test,]$y)

#svmmod = svm(y~., data = img[-test,], kernel = "linear")
```

Linear SVM tuning with 'caret' was much faster.
```{r message = FALSE, warning = FALSE}
#svm with caret
grid = expand.grid(C = 2^(-8:8))
ctrl = trainControl(method='cv', number=5, savePred = TRUE)
svmmod = train(y~., img[-test,], method = "svmLinear", 
               trControl = ctrl, tuneGrid = grid)
```

Plot of CV error of linear SVM
```{r}
#svm plot
ggplot() + 
  geom_line(aes(x = log(svmmod$results[,1]), 
                y = 1-svmmod$results[,2])) + 
  xlab("log(Cost)") + 
  ylab("5-fold CV Error") + 
  geom_vline(aes(xintercept = log(0.125)), linetype = "dotted", 
             colour = "red")

#test error
mean(predict(svmmod, img[test,]) != img[test,]$y)
```


```{r}
#KNN training with caret
grid = expand.grid(k = 1:100)
ctrl = trainControl(method='cv', number=5, savePred = TRUE)
knn.img = train(y~., img[-test,], method = "knn", 
               trControl = ctrl, tuneGrid = grid)
```


```{r}
#plot of KNN CV error
ggplot() +
  geom_line(aes(x = knn.img$results[,1], 
                y = 1 - knn.img$results[,2])) + 
  xlab("K") + 
  ylab("5-fold CV Error")

#compute test error of best knn model
mean(as.vector(predict(knn.img, img[test,])) != img[test,"y"])
```

Random Forest tuning with 'caret.' Range of mtry (variables selected per tree) was 6 ($\sqrt{p}/2$) to 36 ($3\sqrt{p}$) at intervals of 6.
```{r}
#tuning the random forest model using the caret package
grid = expand.grid(.mtry = seq(6, 36, 6),
                   .splitrule = "gini", 
                   .min.node.size = 1)
ctrl = trainControl(method="oob")
rf.img = train(y~., img[-test,], method = "ranger", 
               trControl = ctrl, tuneGrid = grid)
```

```{r}
#random forest graph using oob error
ggplot() + 
  geom_line(aes(x = rf.img$results[,3], 
                y = 1 - rf.img$results[,1])) + 
  xlab("mtry") + ylab("Out-of-bag Error") + 
  geom_vline(aes(xintercept = 12), linetype = "dotted", 
             colour = "red")
```

```{r}
#fit rf model with best mtry based on oob
rf.img1 = ranger(y~., data = img[-test,], 
             mtry = 12, splitrule = "gini", min.node.size = 1)
```

```{r}
#test error of best rf model
mean(predict(rf.img1, img[test,])[["predictions"]] != img[test,]$y)
```


Also used 'ranger' to get a bagging model. 
```{r}
#bagging with ranger package
bag.img = ranger(y~., data = img[-test,], 
             mtry = 144, splitrule = "gini", min.node.size = 1)
```

```{r}
mean(predict(bag.img, img[test,])[["predictions"]] != img[test,]$y)
```