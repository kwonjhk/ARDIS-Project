---
title: "STOR565-Project-PCA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nnet)
library(e1071)
library(class)
library(readr)
library(randomForest)
library(caret)
library(ranger)

#enable multi-core computing
#only comptaible with OSX/Linux
library(doMC)
registerDoMC()

#import data
img <- 
  read_csv("~/Downloads/STOR565/565_project/img_data_32.csv") %>%
  mutate(y = as.factor(y))
names(img) <- make.names(names(img))

#create test set
set.seed(1)
test = sample(1:nrow(img), 600)
```

Principal component analysis of the img_32 data.
```{r}
img.pca <- prcomp(img[,-1025])

#calculate percent variance explained
pV = img.pca$sdev^2/sum(img.pca$sdev^2)

#calculate PC at which cumulative % variance explained is 90%
at90 = which.min(abs(.9 - cumsum(pV)))
```


PCA plots
```{r}
#% variance for each PC
ggplot() + 
  geom_line(aes(x = 1:1024, y = pV)) + 
  xlab("PC") + 
  ylab("% of Variance") +
  ggtitle("PCA % Variance")

#cumulative % variance
ggplot() + 
  geom_line(aes(x = 1:1024, y = cumsum(pV))) + 
  geom_hline(aes(yintercept = 0.9), linetype = "dashed", 
             colour = "red") + 
  geom_vline(aes(xintercept = at90), linetype = "dashed", 
             colour = "red") + 
  xlab("PC") + 
  ylab("% of Variance") +
  ggtitle("PCA Cumulative % Variance")
```

Create dataframe of first 286 PCs
```{r}
pca.df = as.data.frame(img.pca$x[,1:286])  %>%
  cbind(img[,"y"])
```

Graph first two PCs
```{r}
ggplot(pca.df) + 
  geom_point(aes(x = PC1, y = PC2, colour = y), alpha = 0.5)
```

```{r}
pca.df %>%
  filter(y==6 | y==4) %>%
  ggplot() + 
  geom_point(aes(x = PC1, y = PC2, colour = y), alpha = 0.5)
```



```{r}
# multinomial logistic regression model
logmod = multinom(y~., data = pca.df[-test,], MaxNWts = 20000, maxit = 1000)
mean(predict(logmod, pca.df[test,]) != pca.df[test,]$y)
```


```{r}
#svm training with caret
grid = expand.grid(C = 2^(-12:4))
ctrl = trainControl(method='cv', number=5, savePred = TRUE)
svmmod = train(y~., pca.df[-test,], method = "svmLinear", 
               trControl = ctrl, tuneGrid = grid)
```

```{r}
#plot + test error
ggplot() + 
  geom_line(aes(x = log(svmmod$results[,1]), 
                y = 1 - svmmod$results[,2])) + 
  xlab("log(Cost)") + 
  ylab("5-fold CV Error") + 
  geom_vline(aes(xintercept = log(0.00390625)), linetype = "dotted",
             colour = "red")

mean(predict(svmmod, pca.df[test,]) != pca.df[test,]$y)
```



```{r}
#knn training with caret
grid = expand.grid(k = 1:100)
ctrl = trainControl(method='cv', number=5, savePred = TRUE)
knn.img = train(y~., pca.df[-test,], method = "knn", 
               trControl = ctrl, tuneGrid = grid)
```

```{r}
#plot + test error
ggplot() +
  geom_line(aes(x = knn.img$results[,1], 
                y = 1 - knn.img$results[,2])) + 
  xlab("K") + 
  ylab("5-fold CV Error")

mean(predict(knn.img, pca.df[test,]) != pca.df[test,]$y)
```


```{r}
#random forest training with caret
#set grid
#grid values set the same way as in img_12 (look at .rmd for img_12 for specifics).
grid = expand.grid(.mtry = seq(8, 80, 8),
                   .splitrule = "gini", 
                   .min.node.size = 1)
#set control to 5-fold CV
ctrl = trainControl(method="oob")
rf.img = train(y~., pca.df[-test,], method = "ranger", 
               trControl = ctrl, tuneGrid = grid)
```

```{r}
#plot + test error
ggplot() + 
  geom_line(aes(x = rf.img$results[,3], 
                y = 1 - rf.img$results[,1])) + 
  xlab("mtry") + ylab("Out-of-bag Error") + 
  geom_vline(aes(xintercept = 16), linetype = "dotted", 
             colour = "red")
```

```{r}
#fit rf model with best mtry based on oob
rf.img1 = ranger(y~., data = pca.df[-test,], 
             mtry = 16, splitrule = "gini", min.node.size = 1)
```

```{r}
#test error of best rf model
mean(predict(rf.img1, pca.df[test,])[["predictions"]] 
     != pca.df[test,]$y)
```


```{r}
#bagging
bag.img = ranger(y~., data = pca.df[-test,], 
             mtry = 286, splitrule = "gini", min.node.size = 1)
```

```{r}
#test error
mean(predict(bag.img, pca.df[test,])[["predictions"]] != pca.df[test,]$y)
```